"""
test_interactive_demo.py

Unit Tests for Interactive Demo
×™×—×™×“×•×ª ×‘×“×™×§×” ×¢×‘×•×¨ ×“×ž×• ××™× ×˜×¨××§×˜×™×‘×™
"""

import unittest
import sys
import os
from unittest.mock import Mock, patch, MagicMock
import pandas as pd

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class TestDemoConfiguration(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×”×’×“×¨×•×ª ×”×“×ž×•"""
    
    def test_streamlit_components(self):
        """×‘×“×™×§×ª ×¨×›×™×‘×™ Streamlit"""
        expected_components = [
            "text_area",
            "button", 
            "selectbox",
            "tabs",
            "columns",
            "markdown",
            "plotly_chart",
            "dataframe"
        ]
        
        for component in expected_components:
            with self.subTest(component=component):
                self.assertIsInstance(component, str)
                self.assertGreater(len(component), 0)
    
    def test_page_configuration(self):
        """×‘×“×™×§×ª ×”×’×“×¨×ª ×¢×ž×•×“"""
        page_config = {
            "page_title": "Hebrew Sentiment Analysis - Interactive Demo",
            "page_icon": "ðŸŽ­",
            "layout": "wide",
            "initial_sidebar_state": "expanded"
        }
        
        for key, value in page_config.items():
            with self.subTest(config=key):
                self.assertIsNotNone(value)
                self.assertIsInstance(value, str)

class TestMethodsIntegration(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ××™× ×˜×’×¨×¦×™×” ×©×œ ×©×™×˜×•×ª"""
    
    def setUp(self):
        """×”×›× ×” ×œ×‘×“×™×§×•×ª"""
        self.available_methods = [
            "LoRA Fine-tuning",
            "MASK Classification", 
            "Simple Fine-tuning",
            "Simple PEFT",
            "Zero-Shot BART",
            "Advanced Metrics"
        ]
    
    def test_methods_availability(self):
        """×‘×“×™×§×ª ×–×ž×™× ×•×ª ×©×™×˜×•×ª"""
        for method in self.available_methods:
            with self.subTest(method=method):
                self.assertIsInstance(method, str)
                self.assertGreater(len(method), 0)
    
    def test_method_loading_simulation(self):
        """×¡×™×ž×•×œ×¦×™×” ×©×œ ×˜×¢×™× ×ª ×©×™×˜×•×ª"""
        method_status = {}
        
        for method in self.available_methods:
            try:
                # Simulate loading
                method_status[method] = "loaded"
            except Exception as e:
                method_status[method] = f"error: {e}"
        
        # Check that all methods have a status
        self.assertEqual(len(method_status), len(self.available_methods))

class TestUserInterface(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×ž×ž×©×§ ×”×ž×©×ª×ž×©"""
    
    def test_tab_structure(self):
        """×‘×“×™×§×ª ×ž×‘× ×” ×˜××‘×™×"""
        expected_tabs = [
            "ðŸŽ¯ Real-time Prediction",
            "ðŸ“Š Methods Comparison", 
            "ðŸ“ˆ Performance Analysis",
            "ðŸ” Model Details",
            "ðŸ“‹ Results Export"
        ]
        
        self.assertEqual(len(expected_tabs), 5)
        
        for tab in expected_tabs:
            with self.subTest(tab=tab):
                self.assertIsInstance(tab, str)
                self.assertTrue(any(emoji in tab for emoji in ["ðŸŽ¯", "ðŸ“Š", "ðŸ“ˆ", "ðŸ”", "ðŸ“‹"]))
    
    def test_input_validation(self):
        """×‘×“×™×§×ª ××™×ž×•×ª ×§×œ×˜×™×"""
        valid_inputs = [
            "×”×¡×¨×˜ ×”×–×” ×”×™×” × ×”×“×¨",
            "×—×•×•×™×” × ×•×¨××™×ª",
            "×‘×¡×“×¨, ×œ× ×¨×¢ ×•×œ× ×˜×•×‘"
        ]
        
        invalid_inputs = [
            "",  # Empty
            "   ",  # Only spaces
            "English text",  # Non-Hebrew
            "123456"  # Only numbers
        ]
        
        for text in valid_inputs:
            with self.subTest(text=text, type="valid"):
                # Valid Hebrew text
                has_hebrew = any(ord(c) >= 0x0590 and ord(c) <= 0x05FF for c in text)
                self.assertTrue(has_hebrew)
                self.assertGreater(len(text.strip()), 0)
        
        for text in invalid_inputs:
            with self.subTest(text=text, type="invalid"):
                if text.strip() == "":
                    self.assertEqual(len(text.strip()), 0)

class TestVisualization(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×•×™×–×•××œ×™×–×¦×™×”"""
    
    def test_chart_types(self):
        """×‘×“×™×§×ª ×¡×•×’×™ ×’×¨×¤×™×"""
        chart_types = [
            "bar_chart",
            "line_chart", 
            "scatter_plot",
            "heatmap",
            "confusion_matrix",
            "performance_comparison"
        ]
        
        for chart_type in chart_types:
            with self.subTest(chart=chart_type):
                self.assertIsInstance(chart_type, str)
                self.assertGreater(len(chart_type), 0)
    
    def test_metrics_visualization(self):
        """×‘×“×™×§×ª ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×ž×“×“×™×"""
        sample_metrics = {
            "Accuracy": 0.89,
            "Precision": 0.87,
            "Recall": 0.88,
            "F1-Score": 0.87
        }
        
        for metric, value in sample_metrics.items():
            with self.subTest(metric=metric):
                self.assertIsInstance(metric, str)
                self.assertIsInstance(value, float)
                self.assertGreaterEqual(value, 0.0)
                self.assertLessEqual(value, 1.0)

class TestDataExport(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×™×™×¦×•× × ×ª×•× ×™×"""
    
    def test_export_formats(self):
        """×‘×“×™×§×ª ×¤×•×¨×ž×˜×™ ×™×™×¦×•×"""
        export_formats = ["CSV", "Excel", "JSON", "PDF"]
        
        for format_type in export_formats:
            with self.subTest(format=format_type):
                self.assertIsInstance(format_type, str)
                self.assertIn(format_type, ["CSV", "Excel", "JSON", "PDF"])
    
    def test_results_structure(self):
        """×‘×“×™×§×ª ×ž×‘× ×” ×ª×•×¦××•×ª ×œ×™×™×¦×•×"""
        sample_results = {
            "text": "×˜×§×¡×˜ ×œ×“×•×’×ž×”",
            "method": "LoRA Fine-tuning",
            "prediction": "×—×™×•×‘×™",
            "confidence": 0.89,
            "timestamp": "2025-07-11 10:30:00"
        }
        
        required_fields = ["text", "method", "prediction", "confidence", "timestamp"]
        
        for field in required_fields:
            with self.subTest(field=field):
                self.assertIn(field, sample_results)

class TestPerformanceAnalysis(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ × ×™×ª×•×— ×‘×™×¦×•×¢×™×"""
    
    def test_method_comparison(self):
        """×‘×“×™×§×ª ×”×©×•×•××ª ×©×™×˜×•×ª"""
        method_results = {
            "LoRA Fine-tuning": {"accuracy": 0.89, "f1": 0.88},
            "MASK Classification": {"accuracy": 0.85, "f1": 0.84},
            "Simple Fine-tuning": {"accuracy": 0.82, "f1": 0.81},
            "Zero-Shot BART": {"accuracy": 0.78, "f1": 0.77}
        }
        
        # Find best method
        best_method = max(method_results.keys(), 
                         key=lambda m: method_results[m]["accuracy"])
        
        self.assertEqual(best_method, "LoRA Fine-tuning")
        self.assertGreater(method_results[best_method]["accuracy"], 0.85)
    
    def test_performance_trends(self):
        """×‘×“×™×§×ª ×ž×’×ž×•×ª ×‘×™×¦×•×¢×™×"""
        performance_over_time = [
            {"epoch": 1, "accuracy": 0.70},
            {"epoch": 2, "accuracy": 0.75},
            {"epoch": 3, "accuracy": 0.82},
            {"epoch": 4, "accuracy": 0.87},
            {"epoch": 5, "accuracy": 0.89}
        ]
        
        # Check improvement trend
        accuracies = [p["accuracy"] for p in performance_over_time]
        
        for i in range(1, len(accuracies)):
            with self.subTest(epoch=i+1):
                # Generally improving (allowing small drops)
                self.assertGreaterEqual(accuracies[i], accuracies[0])

class TestRealTimePrediction(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×ª×—×–×™×ª ×‘×–×ž×Ÿ ××ž×ª"""
    
    def test_prediction_speed(self):
        """×‘×“×™×§×ª ×ž×”×™×¨×•×ª ×ª×—×–×™×ª"""
        import time
        
        # Simulate prediction time
        start_time = time.time()
        
        # Mock prediction process
        sample_text = "×”×¡×¨×˜ ×”×–×” ×”×™×” × ×”×“×¨"
        prediction = "×—×™×•×‘×™"  # Mock result
        confidence = 0.89
        
        end_time = time.time()
        prediction_time = end_time - start_time
        
        # Should be relatively fast (under 1 second for mock)
        self.assertLess(prediction_time, 1.0)
        self.assertIsInstance(prediction, str)
        self.assertIsInstance(confidence, float)
    
    def test_batch_prediction(self):
        """×‘×“×™×§×ª ×ª×—×–×™×ª ×‘××¦×•×•×”"""
        batch_texts = [
            "×˜×§×¡×˜ ×—×™×•×‘×™ ×¨××©×•×Ÿ",
            "×˜×§×¡×˜ ×©×œ×™×œ×™ ×©× ×™",
            "×˜×§×¡×˜ × ×™×™×˜×¨×œ×™ ×©×œ×™×©×™"
        ]
        
        # Mock batch processing
        batch_results = []
        for i, text in enumerate(batch_texts):
            result = {
                "id": i,
                "text": text,
                "prediction": ["×—×™×•×‘×™", "×©×œ×™×œ×™", "× ×™×™×˜×¨×œ×™"][i],
                "confidence": [0.89, 0.85, 0.82][i]
            }
            batch_results.append(result)
        
        self.assertEqual(len(batch_results), len(batch_texts))
        
        for result in batch_results:
            self.assertIn("text", result)
            self.assertIn("prediction", result)
            self.assertIn("confidence", result)

class TestErrorHandling(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ ×˜×™×¤×•×œ ×‘×©×’×™××•×ª"""
    
    def test_model_loading_errors(self):
        """×‘×“×™×§×ª ×˜×™×¤×•×œ ×‘×©×’×™××•×ª ×˜×¢×™× ×ª ×ž×•×“×œ"""
        error_scenarios = [
            "model_not_found",
            "memory_error",
            "permission_denied",
            "network_error"
        ]
        
        for error in error_scenarios:
            with self.subTest(error=error):
                # Should handle gracefully
                self.assertIsInstance(error, str)
    
    def test_input_sanitization(self):
        """×‘×“×™×§×ª × ×™×§×•×™ ×§×œ×˜×™×"""
        malicious_inputs = [
            "<script>alert('xss')</script>",
            "'; DROP TABLE users; --",
            "../../etc/passwd",
            "null\x00byte"
        ]
        
        for malicious_input in malicious_inputs:
            with self.subTest(input=malicious_input):
                # Should be sanitized
                sanitized = malicious_input.replace("<", "&lt;").replace(">", "&gt;")
                self.assertNotEqual(sanitized, malicious_input)

class TestAccessibility(unittest.TestCase):
    """×‘×“×™×§×•×ª ×¢×‘×•×¨ × ×’×™×©×•×ª"""
    
    def test_rtl_support(self):
        """×‘×“×™×§×ª ×ª×ž×™×›×” ×‘×›×™×•×•×Ÿ ×™×ž×™×Ÿ ×œ×©×ž××œ"""
        hebrew_text = "×˜×§×¡×˜ ×‘×¢×‘×¨×™×ª ×¢× ×›×™×•×•×Ÿ ×™×ž×™×Ÿ ×œ×©×ž××œ"
        
        # Check Hebrew characters present
        has_hebrew = any(ord(c) >= 0x0590 and ord(c) <= 0x05FF for c in hebrew_text)
        self.assertTrue(has_hebrew)
    
    def test_responsive_design(self):
        """×‘×“×™×§×ª ×¢×™×¦×•×‘ ×¨×¡×¤×•× ×¡×™×‘×™"""
        screen_sizes = ["mobile", "tablet", "desktop", "wide"]
        
        for size in screen_sizes:
            with self.subTest(screen=size):
                self.assertIsInstance(size, str)
                self.assertIn(size, ["mobile", "tablet", "desktop", "wide"])

if __name__ == '__main__':
    # ×”×¨×¦×ª ×›×œ ×”×‘×“×™×§×•×ª
    print("ðŸ–¥ï¸ Starting Interactive Demo Tests...")
    print("×ž×ª×—×™×œ ×‘×“×™×§×•×ª ×“×ž×• ××™× ×˜×¨××§×˜×™×‘×™...")
    
    # Create test suite
    test_suite = unittest.TestSuite()
    
    # Add all test classes
    test_classes = [
        TestDemoConfiguration,
        TestMethodsIntegration,
        TestUserInterface,
        TestVisualization,
        TestDataExport,
        TestPerformanceAnalysis,
        TestRealTimePrediction,
        TestErrorHandling,
        TestAccessibility
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # Print summary
    print(f"\nðŸ“Š Test Summary:")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("âœ… All demo tests passed! ×›×œ ×‘×“×™×§×•×ª ×”×“×ž×• ×¢×‘×¨×• ×‘×”×¦×œ×—×”!")
    else:
        print("âŒ Some demo tests failed. ×™×© ×‘×“×™×§×•×ª ×“×ž×• ×©× ×›×©×œ×•.")
