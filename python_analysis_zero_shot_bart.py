# =====================================
# âœ… Zero-Shot BART Evaluation Analysis Script (Final Version)
# =====================================

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ===============================
# ğŸ“¥ 1. Load the dataset
# ===============================
df = pd.read_csv("dataset_zero_shot_bart.csv")
print("âœ… Dataset loaded successfully.")

# Display column names for verification
print("ğŸ“„ Columns:", df.columns)

# ===============================
# ğŸ§¹ 2. Data Cleaning: Filter valid sentiments only
# ===============================
# Keep only rows where label_sentiment is 'positive' or 'negative'
df = df[df["label_sentiment"].isin(["positive", "negative"])]

# ===============================
# ğŸ“Š 3. Basic summary statistics
# ===============================
total = len(df)
correct = (df["label_sentiment"] == df["zero_shot_prediction_bart"]).sum()
accuracy = correct / total * 100

print(f"âœ… Total samples: {total}")
print(f"âœ… Correct predictions: {correct}")
print(f"âœ… Accuracy: {accuracy:.2f}%")

# ===============================
# ğŸ”¢ 4. Count of true labels and predictions
# ===============================
print("\nğŸ”¹ True Sentiment Counts:")
print(df["label_sentiment"].value_counts())

print("\nğŸ”¹ Model Prediction Counts:")
print(df["zero_shot_prediction_bart"].value_counts())

# ===============================
# âŒ 5. Analyze incorrect predictions (errors)
# ===============================
errors = df[df["label_sentiment"] != df["zero_shot_prediction_bart"]]
errors.to_csv("zero_shot_bart_errors.csv", index=False)
print(f"âŒ Total Errors: {len(errors)} (saved to zero_shot_bart_errors.csv)")

# Display first few error examples
print("\nğŸ” Example Errors:")
print(errors.head(10))

# ===============================
# ğŸ“Š 6. Classification Report
# ===============================
print("\nğŸ”¹ Classification Report:")
print(classification_report(df["label_sentiment"], df["zero_shot_prediction_bart"]))

# ===============================
# ğŸŸ¦ 7. Confusion Matrix
# ===============================
cm = confusion_matrix(df["label_sentiment"], df["zero_shot_prediction_bart"], labels=["positive", "negative"])
cm_df = pd.DataFrame(cm, index=["True Positive", "True Negative"], columns=["Pred Positive", "Pred Negative"])

plt.figure(figsize=(6,4))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Zero Shot BART")
plt.savefig("confusion_matrix_zero_shot_bart.png")
plt.show()

print("âœ… Confusion Matrix saved as confusion_matrix_zero_shot_bart.png")

# ===============================
# ğŸ’¾ 8. Save overall summary
# ===============================
summary = {
    "Total Samples": total,
    "Correct Predictions": correct,
    "Accuracy (%)": accuracy,
    "Errors": len(errors)
}

summary_df = pd.DataFrame([summary])
summary_df.to_csv("zero_shot_bart_summary.csv", index=False)

print("âœ… Summary saved as zero_shot_bart_summary.csv")
